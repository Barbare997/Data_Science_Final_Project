{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Pulse - Data Preprocessing\n",
    "\n",
    "## Data Cleaning and Feature Engineering\n",
    "\n",
    "This notebook handles:\n",
    "- Missing value imputation\n",
    "- Outlier detection and handling\n",
    "- Datetime parsing and temporal feature creation\n",
    "- Derived feature engineering (rush hour, traffic stress levels)\n",
    "- Data quality documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:03.872227Z",
     "start_time": "2026-01-08T10:44:03.860652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup project path to import from src\n",
    "from setup_path import setup_project_path\n",
    "setup_project_path()\n",
    "\n",
    "from src.data_processing import (\n",
    "    load_data,\n",
    "    inspect_data,\n",
    "    handle_missing_values,\n",
    "    handle_outliers,\n",
    "    parse_datetime,\n",
    "    create_rush_hour_feature,\n",
    "    create_traffic_stress_level,\n",
    "    preprocess_pipeline,\n",
    "    load_and_clean_data\n",
    ")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "Load the dataset from the exploration notebook or directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:10.491738Z",
     "start_time": "2026-01-08T10:44:10.411985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully: 48204 rows, 9 columns\n",
      "✓ Raw data loaded successfully: (48204, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "data_path = '../data/raw/Metro_Interstate_Traffic_Volume.csv'\n",
    "\n",
    "try:\n",
    "    df_raw = load_data(data_path)\n",
    "    print(f\"✓ Raw data loaded successfully: {df_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️  Data file not found!\")\n",
    "    print(f\"Expected file location: {data_path}\")\n",
    "    print(\"Please ensure the CSV file is in the data/raw/ directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Complete Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline handles all cleaning and feature engineering steps automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Missing Value Handling Strategy - Justification\n",
    "\n",
    "**Forward Fill for numeric/time series columns:**\n",
    "- Time series data: Previous hour's value is realistic for missing weather/traffic data\n",
    "- Preserves all 48,204 observations\n",
    "- Maintains time continuity\n",
    "\n",
    "**Special handling for holiday column:**\n",
    "- 99.87% missing values (48,143/48,204)\n",
    "- Use **mode** (\"None\") instead of forward fill\n",
    "- Reason: Holidays are single-day events; forward fill would incorrectly mark subsequent days as holidays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier Handling Strategy - Justification\n",
    "\n",
    "**Why Cap outliers (IQR method):**\n",
    "- Preserves all 48,204 observations\n",
    "- Maintains time series continuity (no gaps)\n",
    "- Caps extreme values to reasonable limits while keeping data points\n",
    "\n",
    "**Why not remove:**\n",
    "- Would reduce dataset size and break hourly time series structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:15.643642Z",
     "start_time": "2026-01-08T10:44:15.286326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING DATA PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Shape: 48204 rows × 9 columns\n",
      "\n",
      "Missing Values:\n",
      "  holiday: 48143 (99.87%)\n",
      "\n",
      "Duplicate Rows: 17\n",
      "Memory Usage: 11.71 MB\n",
      "============================================================\n",
      "✓ Parsed datetime column 'date_time' and extracted temporal features\n",
      "\n",
      "Handling missing values using 'forward_fill' strategy...\n",
      "  holiday: 48143 -> 0 missing values\n",
      "✓ Capped outliers in 'traffic_volume' at [-4417.00, 10543.00]\n",
      "✓ Created rush hour features\n",
      "✓ Created traffic stress levels:\n",
      "  Low: < 2158\n",
      "  Medium: 2158 - 4586\n",
      "  High: >= 4586\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "Shape: 48204 rows × 19 columns\n",
      "\n",
      "Missing Values:\n",
      "\n",
      "Duplicate Rows: 17\n",
      "Memory Usage: 16.99 MB\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING PIPELINE COMPLETE\n",
      "============================================================\n",
      "Initial rows: 48204\n",
      "Final rows: 48204\n",
      "Features created: 10\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run complete preprocessing pipeline\n",
    "df_processed, preprocessing_report = preprocess_pipeline(\n",
    "    df_raw,\n",
    "    target_column='traffic_volume',\n",
    "    date_column='date_time',\n",
    "    missing_strategy='forward_fill',  # Good for time series data\n",
    "    outlier_method='cap'  # Cap outliers rather than remove\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Preprocessing Results\n",
    "\n",
    "Check that all features were created correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:44:29.738489Z",
     "start_time": "2026-01-08T10:44:29.720824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Features Created:\n",
      "============================================================\n",
      "✓ year\n",
      "    Range: 2012 - 2018\n",
      "✓ month\n",
      "    Range: 1 - 12\n",
      "✓ day\n",
      "    Range: 1 - 31\n",
      "✓ hour\n",
      "    Range: 0 - 23\n",
      "✓ day_of_week\n",
      "    Range: 0 - 6\n",
      "✓ is_weekend\n",
      "    Range: 0 - 1\n",
      "✓ is_rush_hour\n",
      "    Range: 0 - 1\n",
      "✓ rush_hour_type\n",
      "    Values: {'normal': 36147, 'morning_rush': 6177, 'evening_rush': 5880}\n",
      "✓ traffic_stress_level\n",
      "    Values: {'Medium': 16387, 'High': 15910, 'Low': 15907}\n",
      "✓ is_congested\n",
      "    Range: 0 - 1\n"
     ]
    }
   ],
   "source": [
    "# Display new features created\n",
    "print(\"New Features Created:\")\n",
    "print(\"=\"*60)\n",
    "new_features = ['year', 'month', 'day', 'hour', 'day_of_week', 'is_weekend',\n",
    "                'is_rush_hour', 'rush_hour_type', 'traffic_stress_level', 'is_congested']\n",
    "\n",
    "for feature in new_features:\n",
    "    if feature in df_processed.columns:\n",
    "        print(f\"✓ {feature}\")\n",
    "        if df_processed[feature].dtype == 'object':\n",
    "            print(f\"    Values: {df_processed[feature].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(f\"    Range: {df_processed[feature].min()} - {df_processed[feature].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "Save the cleaned and processed dataset for use in EDA and ML notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T10:45:03.671737Z",
     "start_time": "2026-01-08T10:45:03.157235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed data saved to: ../data/processed/traffic_cleaned.csv\n",
      "  Shape: (48204, 19)\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "output_path = '../data/processed/traffic_cleaned.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"✓ Processed data saved to: {output_path}\")\n",
    "print(f\"  Shape: {df_processed.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
